{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import geopandas\n",
    "from shapely.geometry import Polygon\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnx = sqlite3.connect('../us_wildfire_dataset/FPA_FOD_20170508.sqlite')\n",
    "df = pd.read_sql_query(\"SELECT DISCOVERY_DATE, CONT_DATE, LATITUDE, LONGITUDE, STATE, FIRE_NAME, FIRE_SIZE_CLASS, FIRE_SIZE,STAT_CAUSE_DESCR FROM fires\", cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def filter_raw_data(df, state='CA', min_class=None):\n",
    "\n",
    "    # drop states\n",
    "    df_filt = df[df.STATE == 'CA']\n",
    "    df_filt = df_filt.drop(['STATE'], axis=1)\n",
    "\n",
    "    # drop fire classes\n",
    "    if min_class is not None:\n",
    "        df_filt.FIRE_SIZE_CLASS = df_filt.FIRE_SIZE_CLASS.apply(ord)\n",
    "        df_filt = df_filt[df_filt.FIRE_SIZE_CLASS >= ord(min_class)]\n",
    "\n",
    "    # reformat dates\n",
    "    df_filt.DISCOVERY_DATE = pd.to_datetime(df['DISCOVERY_DATE'], unit='D', origin='julian')\n",
    "    df_filt.CONT_DATE = pd.to_datetime(df['CONT_DATE'], unit='D', origin='julian')\n",
    "\n",
    "    # convert coordinates\n",
    "    df_filt = geopandas.GeoDataFrame(df_filt, geometry=geopandas.points_from_xy(\n",
    "        df_filt.LONGITUDE, df_filt.LATITUDE))\n",
    "    df_filt = df_filt.drop(['LONGITUDE'], axis=1)\n",
    "    df_filt = df_filt.drop(['LATITUDE'], axis=1)\n",
    "    df_filt.insert(2, 'COORD', df_filt.pop('geometry'))\n",
    "\n",
    "    # remove missing values\n",
    "    df_filt = df_filt.dropna()\n",
    "\n",
    "    # reformat head\n",
    "    df_filt.columns = [\n",
    "        'start_date', 'end_date', 'geometry',\n",
    "        'name', 'size_class', 'size', 'cause'\n",
    "    ]\n",
    "\n",
    "    # sort by start dates\n",
    "    df_filt = df_filt.sort_values(by='start_date')\n",
    "    df_filt = df_filt.reset_index()\n",
    "\n",
    "    return df_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def extract_geo_fires(df, area):\n",
    "\n",
    "    # return fires within polygon\n",
    "    res = df[df.within(area)]\n",
    "    return res\n",
    "\n",
    "\n",
    "def build_geo_grid(df, grid_area, square_size, verbose=False):\n",
    "    bounds = grid_area.bounds\n",
    "\n",
    "    # calculate number of grids in lat/long directions\n",
    "    long_steps = int((bounds[2] - bounds[0]) / square_size)\n",
    "    lat_steps = int((bounds[3] - bounds[1]) / square_size)\n",
    "\n",
    "    grid_df = []\n",
    "    prog, total = 0, long_steps * lat_steps\n",
    "    for i in range(long_steps):\n",
    "        for j in range(lat_steps):\n",
    "\n",
    "            # update progress\n",
    "            prog += 1\n",
    "            if verbose and prog % 10 == 0:\n",
    "                print('Progress: {}/{}'.format(prog, total), flush=True)\n",
    "            \n",
    "            # get south-east grid square corner\n",
    "            c_lon = bounds[0] + i * square_size\n",
    "            c_lat = bounds[1] + j * square_size\n",
    "\n",
    "            # create grid square\n",
    "            grid = Polygon([\n",
    "                (c_lon, c_lat), \n",
    "                (c_lon + square_size, c_lat), \n",
    "                (c_lon, c_lat + square_size), \n",
    "                (c_lon + square_size, c_lat + square_size)\n",
    "            ])\n",
    "            \n",
    "            fires = extract_geo_fires(df, grid)\n",
    "            grid_df.append([uuid.uuid4(), grid, fires.index])\n",
    "\n",
    "    # build grid df\n",
    "    grid_df = geopandas.GeoDataFrame(grid_df)\n",
    "    grid_df.columns = ['grid_id', 'grid_square', 'fire_indices']\n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Progress: 10/256\nProgress: 20/256\nProgress: 30/256\nProgress: 40/256\nProgress: 50/256\nProgress: 60/256\nProgress: 70/256\nProgress: 80/256\nProgress: 90/256\nProgress: 100/256\nProgress: 110/256\nProgress: 120/256\nProgress: 130/256\nProgress: 140/256\nProgress: 150/256\nProgress: 160/256\nProgress: 170/256\nProgress: 180/256\nProgress: 190/256\nProgress: 200/256\nProgress: 210/256\nProgress: 220/256\nProgress: 230/256\nProgress: 240/256\nProgress: 250/256\n"
    }
   ],
   "source": [
    "# coordinate-square north of San Bernardino/Riverside \n",
    "p = Polygon([(-124, 38), (-124, 42), (-120, 38), (-120, 42)])\n",
    "\n",
    "# filter raw data from dataset and build sexy format\n",
    "df_filt = filter_raw_data(df, state='CA', min_class='C')\n",
    "\n",
    "# df_filt is ordered by start_date with the following columns:\n",
    "#   'start_date', 'end_date', 'geometry', 'name', 'size_class', 'size', 'cause'\n",
    "\n",
    "# build df with each grid square area (from grid area `p` with grid square size 0.1) and corresponding fires indices (which indexes complete list of fires in df_filt)\n",
    "grid_df = build_geo_grid(df_filt, p, 0.25, verbose=True)\n",
    "\n",
    "# grid_df has the following columns:\n",
    "#    'grid_id', 'grid_square', 'fire_indices'\n",
    "#    grid_id: a random uuid to identify the grid square for later\n",
    "#    grid_square: polygon object of the grid square\n",
    "#    fire_indices: list of indices of the fires from df_filt within the grid_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def label_grid_square(df, grid_df, start_date='1992-01', end_date='2015-12', verbose=False):\n",
    "    \n",
    "    # build date range (in months)\n",
    "    date_range = [str(d) for d in np.arange(\n",
    "        start_date, \n",
    "        end_date, \n",
    "        dtype='datetime64[M]'\n",
    "    )]    \n",
    "\n",
    "    # iterate over all grid squares \n",
    "    label_df = []\n",
    "    prog, total = 0, grid_df.shape[0]\n",
    "    for i in range(grid_df.shape[0]):\n",
    "        \n",
    "        # update progress\n",
    "        prog += 1\n",
    "        if verbose and prog % 10 == 0:\n",
    "            print('Progress: {}/{}'.format(prog, total), flush=True)\n",
    "                \n",
    "        # get grid square fires\n",
    "        id = grid_df.loc[i, 'grid_id']\n",
    "        grid_square = grid_df.loc[i, 'grid_square']\n",
    "        fire_indices = list(grid_df.loc[i, 'fire_indices'])\n",
    "        fires = df.loc[fire_indices, :]\n",
    "        \n",
    "        # collect all months in fire date range\n",
    "        months = []\n",
    "        for _, row in fires.iterrows(): \n",
    "            start = row.start_date.date()\n",
    "            end = (row.end_date + pd.DateOffset(months=1)).date()\n",
    "            months.extend([str(d) for d in np.arange(start, end, dtype='datetime64[M]')])\n",
    "        fire_months = list(set(months))\n",
    "        \n",
    "        # label fire months\n",
    "        month_labels = []\n",
    "        for month in date_range:\n",
    "            if month in fire_months: month_labels.append(1)\n",
    "            else: month_labels.append(0)\n",
    "        \n",
    "        # add labels\n",
    "        labels = [id, grid_square] + month_labels\n",
    "        label_df.append(labels)\n",
    "        \n",
    "    # build label df\n",
    "    label_df = pd.DataFrame(label_df)\n",
    "    label_df.columns = ['grid_id', 'grid_square'] + date_range\n",
    "    return label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# labels whether there was a fire in each grid square \n",
    "label_df = label_grid_square(df_filt, grid_df, start_date='2000-01', end_date='2015-12', verbose=False)\n",
    "\n",
    "# label_df has the following columns:\n",
    "#    'grid_id', 'grid_square', 'months ...'\n",
    "#    grid_id: a random uuid to identify the grid square for later\n",
    "#    grid_square: polygon object of the grid square\n",
    "#    indices: the rest of the columns are a label for each month (0 if there was no fire in the grid square during them onth and 1 otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "num_zeros = 0\n",
    "num_subsequent_1s = 0\n",
    "total_iters = 0\n",
    "for index, row in label_df.iterrows():\n",
    "    # because we are using Landsat 7, let's make sure we constrain dates to be from 2000 onwards (hence the + 96)\n",
    "\n",
    "    if np.sum(row[2:]) <= 3: continue\n",
    "    for rowIndex, col in enumerate(row[2 + 96:-1]):\n",
    "        # if not on fire\n",
    "        total_iters += 1\n",
    "        if col == 0:\n",
    "            num_zeros += 1\n",
    "            # include implicit conversion to tuple of bounds instead of polygon shape; will make working with earth engine easier\n",
    "            output.append((row[0], row[1].bounds, label_df.columns[rowIndex + 2 + 96], row[rowIndex + 1 + 2 + 96]))\n",
    "            if row[rowIndex + 1 + 2 + 96] == 1:\n",
    "                num_subsequent_1s += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total iterations: 1504\nTotal data points: 1380\nTotal data points with 1-label: 62\nTotal data points with 0-label: 1318\nPercentage of 1s: 0.045%\n"
    }
   ],
   "source": [
    "print('Total iterations: {}'.format(total_iters))\n",
    "print('Total data points: {}'.format(num_zeros))\n",
    "print('Total data points with 1-label: {}'.format(num_subsequent_1s))\n",
    "print('Total data points with 0-label: {}'.format(num_zeros - num_subsequent_1s))\n",
    "print('Percentage of 1s: {}%'.format(round(num_subsequent_1s / num_zeros * 100, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# save the corresponding data structure to disk\n",
    "import pickle\n",
    "with open(\"../us_wildfire_dataset/labelled_temporal_polygons.pkl\", \"wb\") as f:\n",
    "    pickle.dump(output, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# load output back from the pickle\n",
    "with open(\"../us_wildfire_dataset/labelled_temporal_polygons.pkl\", \"rb\") as f:\n",
    "    unpickler = pickle.Unpickler(f)\n",
    "    output = unpickler.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from calendar import monthrange\n",
    "\n",
    "# change this number 10 to eventually download all of the required images.\n",
    "for index, item in enumerate(output):\n",
    "    uuid, coords, date, label = item\n",
    "    year, month = date.split(\"-\")\n",
    "    year, month = int(year), int(month)\n",
    "\n",
    "    geometry = ee.Geometry.Rectangle(list(coords))\n",
    "\n",
    "    # because we are using LANDSAT7, we need to constrain our dates from 2000 onwards; so far, this has been done above (in cell 96)\n",
    "    collection = ee.ImageCollection(\"LANDSAT/LE07/C01/T1\").filterDate(ee.Date.fromYMD(year, month, 1), ee.Date.fromYMD(year, month, monthrange(year, month)[1])).filterBounds(geometry)\n",
    "    trueColor = collection.select(['B3', 'B2', 'B1'])\n",
    "    col_mean = trueColor.mosaic()\n",
    "\n",
    "    # images in the drive have been named corresponding to their index in the above output data structure\n",
    "    task = ee.batch.Export.image.toDrive(\n",
    "        image = col_mean,             \n",
    "        region = geometry,\n",
    "        description = '%d' % (index),\n",
    "        folder = \"world_images\",\n",
    "        scale = 30\n",
    "    )\n",
    "\n",
    "    task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'state': 'COMPLETED',\n 'description': '886',\n 'creation_timestamp_ms': 1589988656129,\n 'update_timestamp_ms': 1589988692038,\n 'start_timestamp_ms': 1589988661782,\n 'task_type': 'EXPORT_IMAGE',\n 'destination_uris': ['https://drive.google.com/#folders/1MYfA2k6Hh9xjQQE9nSvcX7Lz-RMACeBc'],\n 'id': 'VJU2UMGUZ2VC3XQ7WWS5MVHB',\n 'name': 'projects/earthengine-legacy/operations/VJU2UMGUZ2VC3XQ7WWS5MVHB'}"
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "task.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit96fe36b71316443d9b300233d40611fd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}